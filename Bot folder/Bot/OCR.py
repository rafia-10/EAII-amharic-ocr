

import cv2
import matplotlib.pyplot as plt
import numpy as np
from ultralytics import YOLO
import torch
import torch.nn as nn
import os
import shutil

CRNN_loc = 'best_crnn (1).pth'
yolo_model = YOLO('best (4).pt')
BW_img_loc = 'BW_read_img.jpg'
cropped_imgs_loc = 'cropped_words'

def to_black_and_white(image_path, save_path=None, white_thresh=200):
    """
    Convert an image so that all non-white pixels become black (binary image).

    Args:
        image_path (str): Path to input image.
        save_path (str, optional): Path to save the processed image.
        white_thresh (int): Threshold for "white" (0‚Äì255).
                            Higher = stricter (only pure white remains white).

    Returns:
        np.ndarray: Processed binary image.
    """
    # Read image
    img = cv2.imread(image_path)

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Anything close to white stays white, everything else black
    #_, bw = cv2.threshold(gray, white_thresh, 255, cv2.THRESH_BINARY)

    if save_path:
        cv2.imwrite(save_path, gray)

    return gray


def YOLO_Interface(image_path):
    # 3. Load the image using OpenCV
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Could not load image from {image_path}")
        exit()

    # Convert BGR to RGB for consistent display with matplotlib later
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    display_image = image_rgb.copy() # Create a copy to draw on

    # 4. Perform prediction (no drawing with .plot() here, just get raw results)
    # We set save=False and show=False because we'll handle drawing manually
    results = yolo_model.predict(source=image_path, conf=0.2, iou=0.2, save=False, show=False)
    #results = yolo_model.predict(source=image_path, conf=0.6, iou=0.5, save=False, show=False)

    # 5. Process results and draw boxes manually using OpenCV
    for r in results:
        if r.boxes: # Check if any bounding boxes were detected
            for box in r.boxes:
                # Get coordinates (xyxy format: xmin, ymin, xmax, ymax)
                # .cpu().numpy() converts the tensor to a NumPy array on the CPU
                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())

                # Get class ID and confidence
                class_id = int(box.cls[0].cpu().numpy())
                confidence = float(box.conf[0].cpu().numpy())
                class_name = yolo_model.names[class_id]


                color = (0, 255, 0)
                if yolo_model.task == 'segment': # Example: different color for segmentation if applicable
                    color = (255, 0, 255) # Magenta

                cv2.rectangle(display_image, (x1, y1), (x2, y2), color, 2)

                # Create the label text
                label = f"{class_name} {confidence:.2f}"

                # Calculate text size and position to avoid overlap
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                font_thickness = 2
                text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]

                # Position text above the box, or inside if space is limited
                text_x = x1
                text_y = y1 - 10 # 10 pixels above the top of the box

                # Ensure text is not off-image at the top
                if text_y < 0:
                    text_y = y1 + text_size[1] + 10 # Place below if not enough space above

                # Draw background rectangle for text for better readability
                text_bg_color = (0, 0, 0) # Black background
                #cv2.rectangle(display_image, (text_x, text_y - text_size[1] - 5), # top-left
                 #             (text_x + text_size[0] + 5, text_y + 5), # bottom-right
                  #            text_bg_color, -1) # -1 fills the rectangle

                #cv2.putText(display_image, label, (text_x + 2, text_y), font,
                 #           font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA) # White text

    # 6. Display the annotated image using Matplotlib
    plt.figure(figsize=(12, 10)) # Adjust figure size for better viewing
    plt.imshow(display_image)
    plt.axis('off') # Hide axes
    plt.title('Model Prediction with Custom Labels')
    plt.show()


def YOLO_cropper(image_path, output_folder="cropped_words", row_tolerance=0.6):
    """
    Detect words in an image using YOLO, crop them, and save in left-to-right, top-to-bottom order.
    Groups words into rows based on median y-center positions.

    Args:
        image_path (str): Path to input image.
        output_folder (str): Folder where crops will be saved.
        row_tolerance (float): Fraction of box height allowed for row grouping (default 0.6).
        visualize (bool): Save an annotated image with reading order numbers.

    Returns:
        list: File paths to cropped word images in reading order.
    """
    os.makedirs(output_folder, exist_ok=True)

    results = yolo_model.predict(source=image_path, conf=0.25, iou=0.5, save=False, show=False)
    boxes = results[0].boxes.xyxy.cpu().numpy()

    if len(boxes) == 0:
        print("‚ö†Ô∏è No boxes detected.")
        return []

    # Convert boxes ‚Üí (x1, y1, x2, y2, y_center, height)
    crops_info = []
    for x1, y1, x2, y2 in boxes:
        h = y2 - y1
        yc = (y1 + y2) / 2
        crops_info.append((int(x1), int(y1), int(x2), int(y2), yc, h))

    # Step 1: sort by y_center
    crops_info.sort(key=lambda b: b[4])

    # Step 2: group into rows using median row height
    rows = []
    current_row = [crops_info[0]]
    for box in crops_info[1:]:
        x1, y1, x2, y2, yc, h = box
        _, _, _, _, yc_ref, h_ref = current_row[0]

        # Use median y_center of current row as reference
        row_yc = np.median([b[4] for b in current_row])
        row_h = np.median([b[5] for b in current_row])

        if abs(yc - row_yc) <= row_tolerance * row_h:
            current_row.append(box)
        else:
            rows.append(current_row)
            current_row = [box]
    rows.append(current_row)

    # Step 3: sort each row left-to-right
    for row in rows:
        row.sort(key=lambda b: b[0])

    # Step 4: flatten rows top-to-bottom
    ordered_boxes = [b for row in rows for b in row]

    # Step 5: crop and save
    image = cv2.imread(image_path)
    saved_paths = []
    for idx, (x1, y1, x2, y2, _, _) in enumerate(ordered_boxes, start=1):
        crop = image[y1:y2, x1:x2]
        save_path = os.path.join(output_folder, f"word_{idx}.jpg")
        cv2.imwrite(save_path, crop)
        saved_paths.append(save_path)

    print(f"‚úÖ Saved {len(saved_paths)} crops in reading order to '{output_folder}'")
    return saved_paths

class CRNN(nn.Module):
    def __init__(self, imgH, nc, nclass, nh):
        super(CRNN, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(nc,64,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),
            nn.Conv2d(64,128,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),
            nn.Conv2d(128,256,3,1,1), nn.ReLU(True),
            nn.Conv2d(256,256,3,1,1), nn.ReLU(True), nn.MaxPool2d((2,2),(2,1),(0,1)),
            nn.Conv2d(256,512,3,1,1), nn.BatchNorm2d(512), nn.ReLU(True),
            nn.Conv2d(512,512,3,1,1), nn.BatchNorm2d(512), nn.ReLU(True), nn.MaxPool2d((2,2),(2,1),(0,1)),
            nn.Conv2d(512,512,2,1,0), nn.ReLU(True)
        )
        self.rnn1 = nn.LSTM(512, nh, bidirectional=True, dropout=0.3)
        self.rnn2 = nn.LSTM(nh*2, nh, bidirectional=True, dropout=0.3)
        self.fc = nn.Linear(nh*2, nclass)

    def forward(self, x):
        conv = self.cnn(x)
        b, c, h, w = conv.size()
        conv = conv.squeeze(2).permute(2, 0, 1)
        recurrent, _ = self.rnn1(conv)
        recurrent, _ = self.rnn2(recurrent)
        output = self.fc(recurrent)
        return output.permute(1, 0, 2)

# Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Recreate model with same hyperparameters
CRNN_model = CRNN(imgH=32, nc=1, nclass= 303, nh=256).to(DEVICE)

# Load weights
checkpoint = torch.load(CRNN_loc, map_location=DEVICE)
CRNN_model.load_state_dict(checkpoint)

CRNN_model.eval()
print("Model loaded and ready!")

def preprocess_image(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    print(img.shape)
    h, w, _ = img.shape
    new_w = max(int(32 * (w / h)), 32)
    img = cv2.resize(img, (new_w, 32))
    img = (img / 255.0).astype(np.float32)
    img = (img - 0.5) / 0.5  # normalize
    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # [B,C,H,W]
    return img.to(DEVICE)

BLANK = 302
def ctc_greedy_decoder(output, idx_to_char, blank=BLANK):
    # output: [B, T, nclass]
    preds = output.softmax(2).argmax(2)  # [B, T]
    preds = preds[0].cpu().numpy().tolist()

    decoded = []
    prev = -1
    for p in preds:
        if p != prev and p != blank:  # collapse repeats, ignore blanks
            decoded.append(idx_to_char.get(p, "ÔøΩ"))
        prev = p
    return "".join(decoded)

amharic_mapping = {
    # Basic consonants + vowels
    0: '·àÄ', 1: '·àÅ', 2: '·àÇ', 3: '·àÉ', 4: '·àÑ', 5: '·àÖ', 6: '·àÜ',
    7: '·àà', 8: '·àâ', 9: '·àä', 10: '·àã', 11: '·àå', 12: '·àç', 13: '·àé', 14: '·àè',
    15: '·àê', 16: '·àë', 17: '·àí', 18: '·àì', 19: '·àî', 20: '·àï', 21: '·àñ', 22: '·àó',
    23: '·àò', 24: '·àô', 25: '·àö', 26: '·àõ', 27: '·àú', 28: '·àù', 29: '·àû', 30: '·àü',
    31: '·à†', 32: '·à°', 33: '·à¢', 34: '·à£', 35: '·à§', 36: '·à•', 37: '·à¶', 38: '·àß',
    39: '·à®', 40: '·à©', 41: '·à™', 42: '·à´', 43: '·à¨', 44: '·à≠', 45: '·àÆ', 46: '·àØ',
    47: '·à∞', 48: '·à±', 49: '·à≤', 50: '·à≥', 51: '·à¥', 52: '·àµ', 53: '·à∂', 54: '·à∑',
    55: '·à∏', 56: '·àπ', 57: '·à∫', 58: '·àª', 59: '·àº', 60: '·àΩ', 61: '·àæ', 62: '·àø',
    63: '·âÄ', 64: '·âÅ', 65: '·âÇ', 66: '·âÉ', 67: '·âÑ', 68: '·âÖ', 69: '·âÜ', 70: '·âã',
    71: '·â†', 72: '·â°', 73: '·â¢', 74: '·â£', 75: '·â§', 76: '·â•', 77: '·â¶', 78: '·âß',
    79: '·â®', 80: '·â©', 81: '·â™', 82: '·â´', 83: '·â¨', 84: '·â≠', 85: '·âÆ', 86: '·âØ',
    87: '·â∞', 88: '·â±', 89: '·â≤', 90: '·â≥', 91: '·â¥', 92: '·âµ', 93: '·â∂', 94: '·â∑',
    95: '·â∏', 96: '·âπ', 97: '·â∫', 98: '·âª', 99: '·âº', 100: '·âΩ', 101: '·âæ', 102: '·âø',
    103: '·äÄ', 104: '·äÅ', 105: '·äÇ', 106: '·äÉ', 107: '·äÑ', 108: '·äÖ', 109: '·äÜ',
    110: '·äê', 111: '·äë', 112: '·äí', 113: '·äì', 114: '·äî', 115: '·äï', 116: '·äñ', 117: '·äó',
    118: '·äò', 119: '·äô', 120: '·äö', 121: '·äõ', 122: '·äú', 123: '·äù', 124: '·äû', 125: '·äü',
    126: '·ä†', 127: '·ä°', 128: '·ä¢', 129: '·ä£', 130: '·ä§', 131: '·ä•', 132: '·ä¶', 133: '·äß',
    134: '·ä®', 135: '·ä©', 136: '·ä™', 137: '·ä´', 138: '·ä¨', 139: '·ä≠', 140: '·äÆ', 141: '·äØ',
    142: '·ä∏', 143: '·äπ', 144: '·ä∫', 145: '·äª', 146: '·äº', 147: '·äΩ', 148: '·äæ', 149: '·ãÄ', 150: '·ãÉ',
    151: '·ãà', 152: '·ãâ', 153: '·ãä', 154: '·ãã', 155: '·ãå', 156: '·ãç', 157: '·ãé', 158: '·ãè',
    159: '·ãê', 160: '·ãë', 161: '·ãí', 162: '·ãì', 163: '·ãî', 164: '·ãï', 165: '·ãñ',
    166: '·ãò', 167: '·ãô', 168: '·ãö', 169: '·ãõ', 170: '·ãú', 171: '·ãù', 172: '·ãû', 173: '·ãü',
    174: '·ã†', 175: '·ã°', 176: '·ã¢', 177: '·ã£', 178: '·ã§', 179: '·ã•', 180: '·ã¶', 181: '·ãß',
    182: '·ã®', 183: '·ã©', 184: '·ã™', 185: '·ã´', 186: '·ã¨', 187: '·ã≠', 188: '·ãÆ',
    189: '·ã∞', 190: '·ã±', 191: '·ã≤', 192: '·ã≥', 193: '·ã¥', 194: '·ãµ', 195: '·ã∂', 196: '·ã∑',
    197: '·åÄ', 198: '·åÅ', 199: '·åÇ', 200: '·åÉ', 201: '·åÑ', 202: '·åÖ', 203: '·åÜ', 204: '·åá',
    205: '·åà', 206: '·åâ', 207: '·åä', 208: '·åã', 209: '·åå', 210: '·åç', 211: '·åé', 212: '·åè',
    213: '·å†', 214: '·å°', 215: '·å¢', 216: '·å£', 217: '·å§', 218: '·å•', 219: '·å¶', 220: '·åß',
    221: '·å®', 222: '·å©', 223: '·å™', 224: '·å´', 225: '·å¨', 226: '·å≠', 227: '·åÆ', 228: '·åØ',
    229: '·å∞', 230: '·å±', 231: '·å≤', 232: '·å≥', 233: '·å¥', 234: '·åµ', 235: '·å∂', 236: '·å∑',
    237: '·å∏', 238: '·åπ', 239: '·å∫', 240: '·åª', 241: '·åº', 242: '·åΩ', 243: '·åæ', 244: '·åø',
    245: '·çÄ', 246: '·çÅ', 247: '·çÇ', 248: '·çÉ', 249: '·çÑ', 250: '·çÖ', 251: '·çÜ', 252: '·çá',
    253: '·çà', 254: '·çâ', 255: '·çä', 256: '·çã', 257: '·çå', 258: '·çç', 259: '·çé', 260: '·çè',
    261: '·çê', 262: '·çë', 263: '·çí', 264: '·çì', 265: '·çî', 266: '·çï', 267: '·çñ', 268: '·çó',

    # Special characters / punctuation / numbers
    269: "!", 270: ":-", 271: "<", 272: "(", 273: "¬´", 274: "·ç•", 275: "%", 276: "¬ª", 277: ")",
    278: ">", 279: ".", 280: "+", 281: "·ç£", 282: "-", 283: "·ç¢", 284: "/",
    285: "0", 286: "1", 287: "2", 288: "3", 289: "4", 290: "5", 291: "6", 292: "7", 293: "8", 294: "9",
    295: "·ç°", 296: "·ç§", 297: "...", 298: "*", 299: "#", 300: "?"
}

def CNNR_Interface(img_path):
    img = preprocess_image(img_path)
    ig = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    #plt.imshow(ig)
    #plt.axis('off') # Turn off axis labels and ticks
    #plt.title(img_path)
    #plt.show()
    with torch.no_grad():
        output = CRNN_model(img)  # [B, T, nclass]

    predicted_text = ctc_greedy_decoder(output, amharic_mapping, blank=BLANK)
    print("Predicted:", predicted_text)
    return predicted_text

def clear_folder(folder_path):
    """
    Ensures the folder exists, then deletes all files and subdirectories inside it.

    Args:
        folder_path (str): Path to the folder to clear.
    """
    # Create folder if it does not exist
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"üìÅ Created folder '{folder_path}'")
        return

    # Clear existing contents
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.remove(file_path)  # Remove file or symlink
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)  # Remove subdirectory
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to delete {file_path}. Reason: {e}")

    print(f"‚úÖ Cleared all contents in '{folder_path}'")

def pipeline(img_path, bw = False):
    to_black_and_white(image_path=img_path, save_path= BW_img_loc)
    detected_text = ''
    path = img_path
    if bw:
      path = BW_img_loc
    clear_folder(cropped_imgs_loc)
    print('done cleaning')
    YOLO_Interface(path)
    print('done locating')
    locs = YOLO_cropper(path)
    print('done cropping')
    print(locs)
    if len(locs) == 0:
        return detected_text
    for loc in locs:
        print(loc)
        detected_text += ' '+CNNR_Interface(loc)

    return detected_text

print(pipeline('sample.jpg', bw=True))